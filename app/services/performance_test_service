import time
import subprocess
import os
import csv
import threading
from typing import Any
from app.models.models import PerformanceTest
from flask import current_app

def execute_performance_test(performance_test_id) -> dict[str, str] | dict[str, Any]:
    """
    Executes a Locust performance test and returns the results.

    :param performance_test_id: ID of the PerformanceTest to be executed
    :return: Dictionary containing the results of the performance test
    """
    test = PerformanceTest.query.get(performance_test_id)
    if not test:
        raise ValueError("Test not found")

    locust_command = build_locust_command(test.config)
    result_file_prefix = f"locust_result_{performance_test_id}"

    try:
        subprocess.run(locust_command, check=True, shell=True)
    except subprocess.CalledProcessError as e:
        current_app.logger.error(f"Locust test execution failed: {e}")
        return {'status': 'Error', 'message': str(e)}

    results = parse_locust_test_results(f"{result_file_prefix}_stats.csv")
    return {'status': 'Completed', 'results': results}


def execute_performance_test_async(performance_test_id) -> None:
    """
    Initiates the execution of a performance test asynchronously.

    :param performance_test_id: ID of the PerformanceTest to be executed
    """
    thread = threading.Thread(target=execute_performance_test, args=(performance_test_id,))
    thread.start()


def build_locust_command(config) -> str:
    """
    Builds the command to execute the Locust test based on the test configuration.

    :param config: Dictionary containing the configuration for the Locust test
    :return: String representing the command to execute
    """
    locustfile = config.get('locustfile')
    users = config.get('users', 10)
    spawn_rate = config.get('spawn_rate', 1)
    run_time = config.get('run_time', '1m')
    host = config.get('host')
    result_file_prefix = config.get('result_file_prefix', 'locust_result')

    return (f"locust -f {locustfile} --headless --users {users} --spawn-rate {spawn_rate} "
            f"--run-time {run_time} --host {host} --csv={result_file_prefix}")

def parse_locust_test_results(csv_file) -> list[dict[str | Any, str | Any]] | list:
    """
    Parses the CSV file generated by Locust to a structured format.

    :param csv_file: Path to the CSV file generated by Locust
    :return: Structured dictionary of parsed results
    """
    try:
        with open(csv_file, mode='r') as file:
            csv_reader = csv.DictReader(file)
            result_data = [row for row in csv_reader]
        return result_data
    except IOError as e:
        current_app.logger.error(f"Error reading Locust result file: {e}")
        return []

def aggregate_test_results(results) -> dict:
    """
    Aggregates and analyzes the test results.

    :param results: List of test result data
    :return: Aggregated result data
    """
    # Placeholder for aggregation logic
    # This could involve calculating averages, percentiles, etc.
    aggregated_data = {}
    for result in results:
        # Process each result and update aggregated_data
        pass
    return aggregated_data

import os

def cleanup_test_resources(resource_prefix) -> None:
    """
    Cleans up resources after test execution.

    :param resource_prefix: Prefix used for naming test resources
    """
    for filename in os.listdir('.'):
        if filename.startswith(resource_prefix):
            os.remove(filename)


def schedule_test_execution(test_id, delay):
    """
    Schedules a test execution after a specified delay.

    :param test_id: ID of the test to be executed
    :param delay: Delay in seconds before the test is executed
    """
    def delayed_execution():
        time.sleep(delay)
        execute_performance_test(test_id)

    thread = threading.Thread(target=delayed_execution)
    thread.start()

